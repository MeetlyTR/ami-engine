# 02 — Moral Decision Engine — Mimari Spesifikasyon

**AMI-ENGINE Phase 2: Deterministik Etik Karar Motoru**

Referans: 00_ETHICAL_CONTROL_THEORY.txt, 01_STATE_SPACE_AND_CONSTRAINTS.txt

---

# 1. Bileşen Tanımları

---

## 1.1 State Encoder

**Amaç:** Ham durum bilgisini motorun kullandığı standart state vektörüne dönüştürmek.

**Girdi formatı:**
- raw_state: dict veya struct
  - physical, social, context, risk (dış)
  - compassion, justice, harm_sens, responsibility, empathy (iç)
- Tüm alanlar sayısal veya ölçeklenebilir olmalı.

**Çıktı formatı:**
- x_t: struct/object
  - x_ext: [E_physical, E_social, E_context, E_risk] ∈ [0,1]^4
  - x_moral: [C, J, H, R, E] ∈ [0,1]^5
- Geçersiz/eksik alanlar varsayılan 0.5 veya “bilinmiyor” sabiti ile doldurulur.

**İç mantık:**
- Her alan [0,1]’e clamp + normalize.
- Eğer ham veri kategorikse (örn. “yüksek risk”) önceden tanımlı eşleme tablosu kullanılır.

**Pseudocode:**
```
FUNCTION encode_state(raw_state):
  x_ext = [
    clamp(raw_state.physical, 0, 1),
    clamp(raw_state.social, 0, 1),
    clamp(raw_state.context, 0, 1),
    clamp(raw_state.risk, 0, 1)
  ]
  x_moral = [
    clamp(raw_state.compassion, 0, 1),
    clamp(raw_state.justice, 0, 1),
    clamp(raw_state.harm_sens, 0, 1),
    clamp(raw_state.responsibility, 0, 1),
    clamp(raw_state.empathy, 0, 1)
  ]
  RETURN { x_ext, x_moral }
```

---

## 1.2 Action Generator

**Amaç:** Verilen state için aday aksiyon kümesi A üretmek. Şu aşamada kural tabanlı, deterministik.

**Girdi formatı:**
- x_t: encoded state (x_ext, x_moral)
- config: (opsiyonel) adım sayısı, grid çözünürlüğü

**Çıktı formatı:**
- A: list of action vectors
- Her a = [a_severity, a_compassion, a_intervention, a_delay], her bileşen [0,1].

**İç mantık:**
- Sabit grid: örn. her boyutta 3 değer (0, 0.5, 1) → 3^4 = 81 aksiyon.
- Veya state’e bağlı kısıtlı grid: yüksek risk varsa yüksek severity daha az üretilir (kural tablosu).
- Minimum: “hiçbir şey yapma” a = [0,0,0,1] (delay=max) her zaman dahil.

**Pseudocode:**
```
FUNCTION generate_actions(x_t, resolution=3):
  A = []
  FOR severity IN [0, 0.5, 1]:
    FOR compassion IN [0, 0.5, 1]:
      FOR intervention IN [0, 0.5, 1]:
        FOR delay IN [0, 0.5, 1]:
          a = [severity, compassion, intervention, delay]
          A.APPEND(a)
  A.APPEND([0, 0, 0, 1])  // no-op
  RETURN A
```

---

## 1.3 Moral Evaluation Engine

**Amaç:** Her aday aksiyon a için W(a), J(a), H(a), C(a) skorlarını hesaplamak.

**Girdi formatı:**
- x_t: encoded state
- a: tek bir action vektörü
- (opsiyonel) N, wellbeing_i, lambda_i, rules, vulnerability vb. Phase 1’deki metrik girdileri

**Çıktı formatı:**
- scores: { W, J, H, C } hepsi [0,1] (veya tanım gereği normalize).

**İç mantık:**
- W = Σ λ_i * wellbeing_i(a, x). Basit model: wellbeing_i = 1 − harm_contribution_i; λ_i state’e göre (örn. mağdur ağırlığı artar).
- J = min_k compliance_k(x,a). Her kural için 0/1 veya [0,1] uyum; J = min. Adalet pazarlık dışı.
- H = E[physical + psychological + social damage](x,a). Basit model: H = weighted_avg(severity, intervention, risk_from_state); [0,1]’e normalize.
- C = σ(α*E + β*vulnerability − γ*R). x_moral’daki E, R ve state’ten vulnerability; σ = sigmoid veya clamp.

**Pseudocode:**
```
FUNCTION evaluate_moral(x_t, a):
  W = compute_wellbeing(x_t, a)   // Σ λ_i wellbeing_i, [0,1]
  J = compute_justice(x_t, a)    // min_k compliance_k
  H = compute_harm(x_t, a)       // expected damage, [0,1]
  C = compute_compassion(x_t, a) // σ(αE + βvuln − γR)
  RETURN { W, J, H, C }
```

---

## 1.4 Constraint Validator

**Amaç:** Phase 1 kısıtlarını uygulamak; ihlal eden aksiyonları elemek veya işaretlemek.

**Girdi formatı:**
- scores: { W, J, H, C } (tek aksiyon için)
- constants: J_min=0.85, H_max=0.30, C_min=0.35, C_max=0.75

**Çıktı formatı:**
- valid: boolean
- violations: list of strings (hangi kısıt ihlal)

**İç mantık:**
- J >= J_min AND H <= H_max AND C in [C_min, C_max]. Bir tanesi bile sağlanmazsa valid = false.

**Pseudocode:**
```
FUNCTION validate_constraints(scores, J_min=0.85, H_max=0.30, C_min=0.35, C_max=0.75):
  violations = []
  IF scores.J < J_min: violations.APPEND("J_below_min")
  IF scores.H > H_max: violations.APPEND("H_above_max")
  IF scores.C < C_min OR scores.C > C_max: violations.APPEND("C_out_of_band")
  RETURN { valid: LEN(violations)==0, violations }
```

---

## 1.5 Fail-Safe Controller

**Amaç:** Kritik eşikler aşıldığında tüm optimizasyonu bypass edip güvenli moda geçmek.

**Girdi formatı:**
- scores: { J, H } (veya tüm state + seçilen aksiyonun skorları)
- fail-safe eşikleri: J_critical=0.7, H_critical=0.6

**Çıktı formatı:**
- override: boolean (true = fail-safe aktif)
- safe_action: a = [0, 0.5, 0, 1] gibi sabit “minimum zarar” aksiyonu (a_severity=MIN, a_intervention=SAFE, delay yüksek)
- human_escalation: boolean

**İç mantık:**
- IF J < 0.7 OR H > 0.6 → override = true, safe_action = [0, 0.5, 0, 1], human_escalation = true. Aksi halde override = false.

**Pseudocode:**
```
FUNCTION fail_safe(scores, J_crit=0.7, H_crit=0.6):
  IF scores.J < J_crit OR scores.H > H_crit:
    RETURN { override: true, safe_action: [0, 0.5, 0, 1], human_escalation: true }
  RETURN { override: false, safe_action: null, human_escalation: false }
```

---

## 1.6 Action Selector

**Amaç:** Kısıtları geçen aksiyonlar arasından skor fonksiyonuna göre en iyiyi seçmek. Fail-safe varsa onu döndürmek.

**Girdi formatı:**
- candidates: list of (a, scores) kısıtları geçen aksiyonlar
- fail_safe_result: { override, safe_action }
- scoring weights: α, β, γ, δ; violation_penalty λ

**Çıktı formatı:**
- best_action: a vektörü
- best_score: sayısal
- reason: "fail_safe" | "max_score" | "single_candidate"

**İç mantık:**
- Fail-safe override ise best_action = safe_action, reason = "fail_safe".
- Değilse Score(a) = α*W + β*J − γ*H + δ*C − λ*ViolationPenalty (Phase 1’deki violation’lar zaten elemeyle elendiği için penalty 0). argmax Score(a).

**Pseudocode:**
```
FUNCTION select_action(candidates, fail_safe_result, α, β, γ, δ):
  IF fail_safe_result.override:
    RETURN { action: fail_safe_result.safe_action, score: null, reason: "fail_safe" }
  IF LEN(candidates) == 0:
    RETURN { action: fail_safe_result.safe_action, score: null, reason: "no_valid_fallback" }
  best = MAX(candidates, key = (a,s) -> α*s.W + β*s.J - γ*s.H + δ*s.C)
  RETURN { action: best.a, score: best.score, reason: "max_score" }
```

---

## 1.7 Debug + Trace Logger

**Amaç:** Her adımda girdi/çıktı ve karar gerekçesini kaydetmek; deterministik ve denetlenebilir çıktı üretmek.

**Girdi formatı:**
- Adım adım: encoded_state, action_list, per-action scores, constraint results, fail_safe result, selected action.

**Çıktı formatı:**
- trace: list of events
  - event_type, timestamp (veya step_id), data (state, action, scores, valid, override, selection_reason)
- log: insan okunabilir özet (isteğe bağlı)

**İç mantık:**
- Her bileşen çalıştığında bir trace event eklenir. Sonunda full trace döner. Sıra deterministik olduğu için tekrarlanabilir.

**Pseudocode:**
```
FUNCTION log_trace(trace, step, event_type, data):
  trace.APPEND({ step, event_type, data })
  RETURN trace

// Her bileşen kendi çıktısını trace’e yazar; ana pipeline tüm adımları toplar.
```

---

# 2. Veri Akış Diyagramı (Metin)

```
[ raw_state ]
      |
      v
+------------------+
|  State Encoder   | --> x_t (x_ext, x_moral)
+------------------+
      |
      v
+------------------+
| Action Generator | --> A = [a1, a2, ... an]
+------------------+
      |
      v
+------------------+
| Moral Evaluator  | --> her a için { W, J, H, C }
+------------------+
      |
      v
+------------------+
|Constraint Validator| --> her (a, scores) için valid / violations
+------------------+
      |
      v
  valid (a, scores) listesi
      |
      v
+------------------+
| Fail-Safe Controller | --> override? safe_action? (tüm skorlara bakarak)
+------------------+
      |
      v
+------------------+
| Action Selector  | --> best a (veya safe_action)
+------------------+
      |
      v
[ chosen_action ] + [ full_trace ]
```

---

# 3. Çalıştırma Boru Hattı (Adım Adım)

1. **Encode:** raw_state → State Encoder → x_t. Trace: state_encoded.
2. **Generate:** x_t → Action Generator → A. Trace: actions_generated, |A|.
3. **Evaluate:** Her a ∈ A için Moral Evaluator(x_t, a) → scores(a). Trace: per (a, scores).
4. **Filter:** Her (a, scores) için Constraint Validator(scores) → valid/violations. Geçerli olanları candidates listesine al. Trace: constraint_results, candidates.
5. **Fail-Safe:** Tüm skorlara (veya seçilen adayın skoruna) göre Fail-Safe Controller. override ise 6’yı atla, safe_action dön. Trace: fail_safe_result.
6. **Select:** candidates + fail_safe_result → Action Selector → best_action, reason. Trace: selected_action, reason.
7. **Return:** best_action + full_trace. Trace Logger tüm adımları zaten toplamıştır.

---

# 4. Deterministik Skorlama Akışı

- State Encoder: sabit clamp + varsayılanlar → aynı raw_state → aynı x_t.
- Action Generator: sabit grid veya kural → aynı x_t → aynı A sırası.
- Moral Evaluator: sabit formüller (W, J, H, C) → aynı (x_t, a) → aynı scores.
- Constraint Validator: sabit eşikler → aynı scores → aynı valid.
- Fail-Safe: sabit J_crit, H_crit → aynı skorlar → aynı override.
- Action Selector: argmax tek ve sabit → aynı candidates → aynı best_action.

Sonuç: Aynı raw_state her zaman aynı chosen_action ve aynı trace’i üretir.

---

# 5. Uçtan Uca Pseudocode

```
FUNCTION moral_decision_engine(raw_state, config):
  trace = []

  x_t = StateEncoder(raw_state)
  trace = log_trace(trace, 1, "state_encoded", x_t)

  A = ActionGenerator(x_t, config.resolution)
  trace = log_trace(trace, 2, "actions_generated", { count: LEN(A), actions: A })

  scored = []
  FOR a IN A:
    scores = MoralEvaluationEngine(x_t, a)
    scored.APPEND((a, scores))
  trace = log_trace(trace, 3, "moral_scores", scored)

  candidates = []
  FOR (a, scores) IN scored:
    cv = ConstraintValidator(scores)
    IF cv.valid:
      candidates.APPEND((a, scores))
    trace = log_trace(trace, 4, "constraint", { a, scores, valid: cv.valid, violations: cv.violations })

  // Fail-safe: seçilen adayların en kötü J/H'ına veya tüm skorlara bak
  worst_J = MIN( s.J FOR (a,s) IN scored )
  worst_H = MAX( s.H FOR (a,s) IN scored )
  fs = FailSafeController({ J: worst_J, H: worst_H })
  trace = log_trace(trace, 5, "fail_safe", fs)

  sel = ActionSelector(candidates, fs, config.α, config.β, config.γ, config.δ)
  trace = log_trace(trace, 6, "selection", { action: sel.action, reason: sel.reason, score: sel.score })

  RETURN { action: sel.action, trace: trace, human_escalation: fs.human_escalation }
```

---

# 6. Örnek Test Senaryosu

**Senaryo:** Acil durum; bir kişi risk altında, yüksek müdahale hızı gerekiyor. Düşük adalet ihlali riski.

**raw_state (örnek):**
- physical = 0.8 (tehlike yüksek)
- social = 0.7 (bir mağdur ön planda)
- context = 0.6 (acil tıbbi/hukuki bağlam)
- risk = 0.75
- compassion = 0.6, justice = 0.9, harm_sens = 0.7, responsibility = 0.8, empathy = 0.65

**Beklenen davranış:**
- Fail-safe tetiklenmemeli (J yüksek, H makul tutulabilir).
- Yüksek intervention, orta-yüksek severity, düşük delay beklenir.
- Seçilen aksiyon: örn. [0.5, 0.5, 0.8, 0.2] gibi (müdahale et, erteleme düşük).
- Trace’te tüm adımlar, her a için W,J,H,C, constraint valid/violations, selection reason görülmeli.

**Alternatif senaryo (fail-safe):**
- raw_state’te justice çok düşük veya harm çok yüksek simüle edilirse (örn. J=0.65, H=0.7) → override=true, safe_action=[0, 0.5, 0, 1], human_escalation=true.

---

# 7. Beklenen Çıktılar (Özet)

- **action:** [a_severity, a_compassion, a_intervention, a_delay] ∈ [0,1]^4.
- **trace:** Adım numarası, event_type, ilgili data (state, actions, scores, valid, override, selection).
- **human_escalation:** boolean; true ise insan onayı/eskalasyon akışı tetiklenir.
- Tüm kararlar skor formülleri ve kısıt/fail-safe kuralları ile **matematiksel olarak açıklanabilir**; black-box yok.

---

Bu spesifikasyon Phase 1’deki state space, kısıtlar ve metriklerle uyumludur. Phase 2 implementasyonu bu belgeye göre kodlanabilir ve birim testleri yazılabilir.
