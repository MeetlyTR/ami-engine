# AMI-ENGINE — Codebase Cleanup Analysis Report

**Date:** 2026-02-13  
**Scope:** Remove/consolidate unnecessary, unused, or redundant elements WITHOUT changing model, architecture, behavior, outputs, or test results.

---

## Step 1: Analysis Report (no code changes)

### 1.1 Repository structure (Python)

- **Root:** engine.py, config.py, run_all_tests.py
- **core/:** state_encoder, action_generator, moral_evaluator, constraint_validator, fail_safe, action_selector, trace_logger, confidence, uncertainty, soft_clamp, soft_override, temporal_drift, trace_collector
- **config_profiles/:** base, production_safe, high_critical, chaos_tuning, scenario_test, clamp_test — all referenced by config_profiles/__init__.py (get_config, list_profiles)
- **learning/:** feedback_metrics, loss, safety_gate, policy_optimizer, offline_loop, run_optimization_loop — all used (engine path, tools, dashboard, tests)
- **simulation/:** scenario_generator — used by dashboard, run_optimization_loop, inspect_dashboard_data, test_dashboard_scenarios, test_scenario_generator, run_offline_learning
- **visualization/:** dashboard, i18n, plots/* — all used
- **tools/:** run_offline_learning.py, tune_thresholds.py — documented entry points; used
- **tests/:** All test modules are invoked by run_all_tests.py or run_model_test.bat or documented as standalone (inspect_dashboard_data, run_example)

### 1.2 Unused files

- **None.** Every .py file is either:
  - Imported by run_all_tests.py or by another module, or
  - Run as __main__ (dashboard, engine, run_offline_learning, tune_thresholds, run_example, inspect_dashboard_data, test_dashboard_scenarios, adversarial/chaos runners), or
  - Part of a package __init__ (core, learning, simulation, config_profiles, tests.*).

### 1.3 Unused functions / classes

- **TraceLogger, TraceEvent:** Used in engine.py (logger.log, logger.trace).
- **compute_confidence, ConfidenceResult:** Used in engine.py and tests.
- **safety_gate:** Used in learning/run_optimization_loop.py.
- **extract_selection_data, replay:** Used in tests and engine (replay).
- **run_offline_step, run_engine_on_states:** Used by run_optimization_loop and tools/run_offline_learning.
- **Monte Carlo generator.generate_batch** vs **simulation.scenario_generator.generate_batch:** Different semantics (uniform random vs profile-based); both needed. No consolidation.

No dead functions or classes identified.

### 1.4 Duplicate utilities

- **load_traces_from_jsonl:** Implemented identically in:
  - `visualization/dashboard.py` (local function, returns list)
  - `learning/feedback_metrics.py` (returns List[Dict[str, Any]])
  Logic: read JSONL, strip lines, json.loads each line, append to list, return. Behavior is identical. Dashboard already depends on learning when user runs optimization (run_optimization_loop). Consolidation: have dashboard use learning.feedback_metrics.load_traces_from_jsonl and remove the local copy.

### 1.5 Unused imports

- **tests/inspect_dashboard_data.py:** `import os` — never used (no os.* in file). Safe to remove.

No other unused imports were confirmed in the scanned set (engine, core, config_profiles, learning, run_all_tests, dashboard, key tests).

### 1.6 Legacy artifacts (old specs, demos, scripts)

- **docs/:** Various .txt specs and notes reference current scripts (inspect_dashboard_data, run_example, run_offline_learning, tune_thresholds). No obsolete script or duplicate test runner identified.
- **run_model_test.bat:** Runs test_dashboard_scenarios.py (complement to run_all_tests.py). Kept.
- **tests/monte_carlo/run_example.py:** Documented in RUN_TESTS.md; optional standalone. Kept.

No removal of docs or scripts proposed.

### 1.7 Commented-out code

- No large blocks of commented-out experimental code found. One grep hit was a comment line in tools/tune_thresholds.py (description), not commented code.

### 1.8 Summary

- **Unused files:** 0
- **Unused public functions/classes:** 0
- **Duplicate utility (same behavior):** 1 — load_traces_from_jsonl (dashboard vs feedback_metrics)
- **Unused imports:** 1 — os in tests/inspect_dashboard_data.py
- **Legacy scripts to remove:** 0
- **Commented-out blocks to remove:** 0

---

## Step 2: Proposed deletions / consolidations

1. **Remove unused import**
   - **File:** `tests/inspect_dashboard_data.py`
   - **Change:** Remove the line `import os`.
   - **Reason:** `os` is never used in the file.

2. **Consolidate load_traces_from_jsonl**
   - **File:** `visualization/dashboard.py`
   - **Change:**
     - Remove the local function `load_traces_from_jsonl` (entire definition, ~15 lines).
     - Add at top with other imports: `from learning.feedback_metrics import load_traces_from_jsonl` (or add to existing learning import block if present).
   - **Reason:** Identical implementation; single source of truth in learning.feedback_metrics. No behavior or test result change; dashboard already uses learning when optimization is run.

**No other deletions or consolidations proposed.** All other modules and exports are referenced; Monte Carlo vs scenario generate_batch serve different purposes; safety_gate, TraceLogger, confidence, etc. are all in use.

---

## Step 3: Await approval

After your confirmation, the two changes above will be applied in small, safe steps. If you prefer to skip consolidation (e.g. to keep visualization free of learning dependency for JSONL loading), only the unused `import os` removal can be applied.

Safety > cleanliness: anything ambiguous was left unchanged.
